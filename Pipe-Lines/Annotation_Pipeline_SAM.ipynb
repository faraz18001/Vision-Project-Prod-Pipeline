{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SAM-Enhanced Auto-Labeling Pipeline\n",
                "\n",
                "**Description**: Uses YOLO for detection + SAM for precise segmentation.\n",
                "\n",
                "**How it works**:\n",
                "1. YOLO detects objects (Hardhat, Person, Vest)\n",
                "2. SAM refines each detection with pixel-perfect masks\n",
                "3. Tight bounding boxes are extracted from masks\n",
                "\n",
                "**Result**: More accurate bounding boxes than YOLO alone."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: Install Dependencies\n",
                "!pip install ultralytics segment-anything opencv-python-headless"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 2a: Upload Model\n",
                "import os\n",
                "from google.colab import files\n",
                "\n",
                "os.makedirs(\"models\", exist_ok=True)\n",
                "\n",
                "print(\"--- UPLOAD MODEL ---\")\n",
                "print(\"Upload your 'best.pt' model file.\")\n",
                "\n",
                "uploaded_model = files.upload()\n",
                "\n",
                "MODEL_PATH = \"yolov8n.pt\"\n",
                "\n",
                "for filename in uploaded_model.keys():\n",
                "    if filename.endswith('.pt'):\n",
                "        print(f\"Model detected: {filename}\")\n",
                "        os.rename(filename, os.path.join(\"models\", filename))\n",
                "        MODEL_PATH = os.path.join(\"models\", filename)\n",
                "\n",
                "if MODEL_PATH == \"yolov8n.pt\":\n",
                "    print(\"WARNING: No .pt file uploaded. Using default.\")\n",
                "else:\n",
                "    print(f\"Model loaded: {MODEL_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 2b: Upload Images\n",
                "import zipfile\n",
                "\n",
                "os.makedirs(\"input_images\", exist_ok=True)\n",
                "IMAGES_DIR = \"input_images\"\n",
                "\n",
                "print(\"--- UPLOAD IMAGES ---\")\n",
                "print(\"Upload a ZIP file containing your images.\")\n",
                "\n",
                "uploaded_images = files.upload()\n",
                "\n",
                "for filename in uploaded_images.keys():\n",
                "    if filename.endswith('.zip'):\n",
                "        print(f\"Extracting: {filename}\")\n",
                "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
                "            zip_ref.extractall(IMAGES_DIR)\n",
                "        print(f\"Extracted to: {IMAGES_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 3: Load YOLO and SAM Models\n",
                "from ultralytics import YOLO\n",
                "from segment_anything import sam_model_registry, SamPredictor\n",
                "import torch\n",
                "\n",
                "# Load YOLO\n",
                "print(\"Loading YOLO...\")\n",
                "yolo = YOLO(MODEL_PATH)\n",
                "\n",
                "# Download and Load SAM\n",
                "SAM_CHECKPOINT = \"sam_vit_h_4b8939.pth\"\n",
                "if not os.path.exists(SAM_CHECKPOINT):\n",
                "    print(\"Downloading SAM checkpoint (~2.5GB)...\")\n",
                "    !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
                "    print(\"Downloaded!\")\n",
                "\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Device: {DEVICE}\")\n",
                "\n",
                "print(\"Loading SAM...\")\n",
                "sam = sam_model_registry[\"vit_h\"](checkpoint=SAM_CHECKPOINT)\n",
                "sam.to(device=DEVICE)\n",
                "predictor = SamPredictor(sam)\n",
                "\n",
                "print(\"Both models loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 4: SAM-Enhanced Auto-Labeling\n",
                "import cv2\n",
                "import numpy as np\n",
                "import shutil\n",
                "\n",
                "OUTPUT_ROOT = \"output_data\"\n",
                "IMAGES_OUT = os.path.join(OUTPUT_ROOT, \"images\")\n",
                "LABELS_OUT = os.path.join(OUTPUT_ROOT, \"labels\")\n",
                "\n",
                "os.makedirs(IMAGES_OUT, exist_ok=True)\n",
                "os.makedirs(LABELS_OUT, exist_ok=True)\n",
                "\n",
                "# Target classes\n",
                "NEW_CLASSES = ['Hardhat', 'Person', 'Safety Vest']\n",
                "with open(os.path.join(OUTPUT_ROOT, \"classes.txt\"), \"w\") as f:\n",
                "    for c in NEW_CLASSES:\n",
                "        f.write(c + \"\\n\")\n",
                "\n",
                "OLD_NAMES = yolo.names\n",
                "\n",
                "def get_bbox_from_mask(mask):\n",
                "    \"\"\"Extract tight bounding box from SAM mask.\"\"\"\n",
                "    rows = np.any(mask, axis=1)\n",
                "    cols = np.any(mask, axis=0)\n",
                "    if not np.any(rows) or not np.any(cols):\n",
                "        return None\n",
                "    y_min = np.where(rows)[0][0]\n",
                "    y_max = np.where(rows)[0][-1]\n",
                "    x_min = np.where(cols)[0][0]\n",
                "    x_max = np.where(cols)[0][-1]\n",
                "    return x_min, y_min, x_max, y_max\n",
                "\n",
                "# Find images\n",
                "files_list = []\n",
                "for root, dirs, files in os.walk(IMAGES_DIR):\n",
                "    for file in files:\n",
                "        if file.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
                "            files_list.append(os.path.join(root, file))\n",
                "\n",
                "print(f\"Processing {len(files_list)} images with SAM refinement...\")\n",
                "\n",
                "count = 0\n",
                "\n",
                "for img_path in files_list:\n",
                "    filename = os.path.basename(img_path)\n",
                "    \n",
                "    # Read image\n",
                "    image_bgr = cv2.imread(img_path)\n",
                "    if image_bgr is None:\n",
                "        continue\n",
                "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
                "    h, w = image_bgr.shape[:2]\n",
                "    \n",
                "    # Set image for SAM\n",
                "    predictor.set_image(image_rgb)\n",
                "    \n",
                "    # YOLO Detection\n",
                "    results = yolo.predict(img_path, conf=0.25, verbose=False)\n",
                "    boxes = results[0].boxes\n",
                "    \n",
                "    if len(boxes) == 0:\n",
                "        continue\n",
                "    \n",
                "    valid_lines = []\n",
                "    \n",
                "    for box in boxes:\n",
                "        old_id = int(box.cls[0])\n",
                "        old_name = OLD_NAMES[old_id]\n",
                "        \n",
                "        # Filter classes\n",
                "        new_id = -1\n",
                "        if old_name == 'Hardhat':\n",
                "            new_id = 0\n",
                "        elif old_name == 'Person':\n",
                "            new_id = 1\n",
                "        elif old_name == 'Safety Vest':\n",
                "            new_id = 2\n",
                "        \n",
                "        if new_id == -1:\n",
                "            continue\n",
                "        \n",
                "        # Get YOLO box coordinates (xyxy format)\n",
                "        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
                "        \n",
                "        # Use box center as SAM prompt\n",
                "        center_x = int((x1 + x2) / 2)\n",
                "        center_y = int((y1 + y2) / 2)\n",
                "        \n",
                "        # SAM prediction using center point\n",
                "        input_point = np.array([[center_x, center_y]])\n",
                "        input_label = np.array([1])  # 1 = foreground\n",
                "        \n",
                "        masks, scores, _ = predictor.predict(\n",
                "            point_coords=input_point,\n",
                "            point_labels=input_label,\n",
                "            multimask_output=True,\n",
                "        )\n",
                "        \n",
                "        # Use best mask\n",
                "        best_idx = np.argmax(scores)\n",
                "        mask = masks[best_idx]\n",
                "        \n",
                "        # Get refined bounding box from mask\n",
                "        refined_bbox = get_bbox_from_mask(mask)\n",
                "        \n",
                "        if refined_bbox is None:\n",
                "            # Fall back to YOLO box\n",
                "            bx = box.xywhn[0].cpu().numpy()\n",
                "            valid_lines.append(f\"{new_id} {bx[0]:.6f} {bx[1]:.6f} {bx[2]:.6f} {bx[3]:.6f}\")\n",
                "        else:\n",
                "            # Use SAM-refined box (convert to YOLO format)\n",
                "            rx1, ry1, rx2, ry2 = refined_bbox\n",
                "            x_center = ((rx1 + rx2) / 2) / w\n",
                "            y_center = ((ry1 + ry2) / 2) / h\n",
                "            box_w = (rx2 - rx1) / w\n",
                "            box_h = (ry2 - ry1) / h\n",
                "            valid_lines.append(f\"{new_id} {x_center:.6f} {y_center:.6f} {box_w:.6f} {box_h:.6f}\")\n",
                "    \n",
                "    # Save if valid objects found\n",
                "    if valid_lines:\n",
                "        shutil.copy(img_path, os.path.join(IMAGES_OUT, filename))\n",
                "        txt_name = os.path.splitext(filename)[0] + \".txt\"\n",
                "        with open(os.path.join(LABELS_OUT, txt_name), \"w\") as f:\n",
                "            f.write(\"\\n\".join(valid_lines))\n",
                "        count += 1\n",
                "        if count % 10 == 0:\n",
                "            print(f\"Processed {count} images...\")\n",
                "\n",
                "print(f\"\\nDone! {count} images with SAM-refined labels.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 5: Download Results\n",
                "!zip -r sam_labels.zip output_data\n",
                "\n",
                "from google.colab import files\n",
                "files.download('sam_labels.zip')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}